{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "qmkj-80IHxnd"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# Simple EncoderDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements = \"\"\"\n",
    "# keras\n",
    "# git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# matplotlib\n",
    "# numpy\n",
    "# scipy\n",
    "# pillow\n",
    "# #urllib\n",
    "# #skimage\n",
    "# scikit-image\n",
    "# #gzip\n",
    "# #pickle\n",
    "# \"\"\"\n",
    "# %store requirements > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# !pip install tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from random import shuffle\n",
    "import PIL\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# matplotlib.use('TKAgg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "Steps to generate URL used below:\n",
    "- Say, your data files are in the directory called 'input'\n",
    "- Manually create a zip file, 'input.zip'\n",
    "- Sync it to gDrive\n",
    "- In gDrive, Share it with Public model, copy its share-able link\n",
    "- Use https://sites.google.com/site/gdocs2direct/ to generate corresponding Direct link\n",
    "- Paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Continuum\\anaconda3\\envs\\odsc_ws\\lib\\site-packages\\PIL\\Image.py\n",
      "C:\\Continuum\\anaconda3\\envs\\odsc_ws\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "# test PIL installation\n",
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image\n",
    "\n",
    "# both should point to same dir\n",
    "from PIL import Image\n",
    "print(Image.__file__)\n",
    "\n",
    "import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Continuum\\\\anaconda3\\\\envs\\\\odsc_ws\\\\notebooks\\\\MidcurveNN-master'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working directory\n",
    "wdir = os.getcwd()\n",
    "wdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "height": 68
    },
    "colab_type": "code",
    "id": "Kn-k8kTXuAlv",
    "outputId": "cd1649cb-4f17-4857-a137-e7b1a5e1d002"
   },
   "outputs": [],
   "source": [
    "# _URL = 'https://drive.google.com/uc?export=download&id=16rqDFLO__WySSQGlAht0FEj2uJZg4M9M'\n",
    "\n",
    "# path_to_zip = tf.keras.utils.get_file('input.zip',\n",
    "#                                       origin=_URL,\n",
    "#                                       extract=True)\n",
    "\n",
    "# input_data_folder = os.path.join(os.path.dirname(path_to_zip), 'input')\n",
    "#input_data_folder = \"D:/Yogesh/ToDos/Research/MidcurveNN/code/data/input\"\n",
    "input_data_folder = wdir + \"\\data\\input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_image_pairs(datafolder=input_data_folder):\n",
    "    profile_pngs = []\n",
    "    midcurve_pngs = []\n",
    "    for file in os.listdir(datafolder):\n",
    "        fullpath = os.path.join(datafolder, file)\n",
    "        if os.path.isdir(fullpath):\n",
    "            continue\n",
    "        if file.endswith(\".png\"):\n",
    "            if file.find(\"Profile\") != -1:\n",
    "                profile_pngs.append(fullpath)\n",
    "            if file.find(\"Midcurve\") != -1:\n",
    "                midcurve_pngs.append(fullpath)\n",
    "    profile_pngs = sorted(profile_pngs)\n",
    "    midcurve_pngs = sorted(midcurve_pngs)\n",
    "    return profile_pngs,midcurve_pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CbTEt448b4R"
   },
   "outputs": [],
   "source": [
    "def get_training_data(datafolder = input_data_folder):\n",
    "    profile_pngs,midcurve_pngs = read_input_image_pairs(datafolder)\n",
    "    \n",
    "    profile_pngs_objs = [img_to_array(load_img(f, color_mode='rgba', target_size=(100, 100))) for f in profile_pngs ]\n",
    "    midcurve_pngs_objs = [img_to_array(load_img(f, color_mode='rgba', target_size=(100, 100))) for f in midcurve_pngs]\n",
    "\n",
    "#     profile_pngs_objs = np.array([x.reshape((1,) + x.shape) for x in profile_pngs_objs])\n",
    "#     midcurve_pngs_objs = np.array([x.reshape((1,) + x.shape) for x in midcurve_pngs_objs])\n",
    "\n",
    "    profile_pngs_gray_objs = [x[:,:,3] for x in profile_pngs_objs]\n",
    "    midcurve_pngs_gray_objs =[x[:,:,3] for x in midcurve_pngs_objs]\n",
    "    \n",
    "#     profile_pngs_gray_objs = [np.where(x>128, 0, 1) for x in profile_pngs_gray_objs]\n",
    "#     midcurve_pngs_gray_objs =[np.where(x>128, 0, 1) for x in midcurve_pngs_gray_objs]\n",
    "        \n",
    "    # shufle them\n",
    "    zipped_profiles_midcurves = [(p,m) for p,m in zip(profile_pngs_gray_objs,midcurve_pngs_gray_objs)]\n",
    "    shuffle(zipped_profiles_midcurves)\n",
    "    profile_pngs_gray_objs, midcurve_pngs_gray_objs = zip(*zipped_profiles_midcurves)\n",
    "    \n",
    "    return profile_pngs_gray_objs, midcurve_pngs_gray_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "height": 558
    },
    "colab_type": "code",
    "id": "4OLHMpsQ5aOv",
    "outputId": "5bfde74b-a0dd-42fa-8e9c-4f6eb42215ed"
   },
   "outputs": [],
   "source": [
    "profile_pngs_objs, midcurve_pngs_objs = get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(original_imgs,computed_imgs):\n",
    "    n = 10  # how many digits we will display\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original_imgs[i].reshape(100, 100),cmap='gray_r')\n",
    "#         plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(computed_imgs[i].reshape(100, 100),cmap='gray_r')\n",
    "#         plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(profile_pngs_objs,midcurve_pngs_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from prepare_data import get_training_data\n",
    "from prepare_plots import plot_results\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "class simple_encoderdecoder:\n",
    "    def __init__(self):\n",
    "        self.encoding_dim = 100\n",
    "        self.input_dim = 10000\n",
    "        self.epochs = 200\n",
    "        self.autoencoder_model_pkl = wdir + \"\\data\\models\\autoencoder_model.pkl\"\n",
    "        self.encoder_model_pkl = wdir + \"\\data\\models\\encoder_model.pkl\"\n",
    "        self.decoder_model_pkl = wdir + \"\\data\\models\\decoder_model.pkl\"\n",
    "                \n",
    "    def process_images(self,grayobjs):\n",
    "        flat_objs = [x.reshape(self.input_dim) for x in grayobjs]\n",
    "        pngs_objs = np.array(flat_objs)\n",
    "        return pngs_objs\n",
    "\n",
    "    def train(self,\n",
    "            profile_pngs_gray_objs, \n",
    "            midcurve_pngs_gray_objs):\n",
    "        \n",
    "        if not os.path.exists(self.autoencoder_model_pkl):\n",
    "            # this is our input placeholder\n",
    "            input_img = Input(shape=(self.input_dim,))\n",
    "            \n",
    "            # \"encoded\" is the encoded representation of the input\n",
    "            encoded = Dense(self.encoding_dim, activation='relu',activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "            # \"decoded\" is the lossy reconstruction of the input\n",
    "            decoded = Dense(self.input_dim, activation='sigmoid')(encoded) \n",
    "            \n",
    "            # Model 1: Full AutoEncoder, includes both encoder single dense layer and decoder single dense layer. \n",
    "            # This model maps an input to its reconstruction\n",
    "            self.autoencoder = Model(input_img, decoded)\n",
    "                    \n",
    "            # Model 2: a separate encoder model: -------------------\n",
    "            # this model maps an input to its encoded representation\n",
    "            self.encoder = Model(input_img, encoded)\n",
    "            \n",
    "            # Model 3: a separate encoder model: -------------------\n",
    "            # create a placeholder for an encoded (32-dimensional) input\n",
    "            encoded_input = Input(shape=(self.encoding_dim,))\n",
    "            # retrieve the last layer of the autoencoder model\n",
    "            decoder_layer = self.autoencoder.layers[-1]\n",
    "            # create the decoder model\n",
    "            self.decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "            \n",
    "            # Compilation of Autoencoder (only)\n",
    "            self.autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "            \n",
    "#             # Training\n",
    "#             profile_pngs_flat_objs = [x.reshape(self.input_dim) for x in profile_pngs_gray_objs]\n",
    "#             midcurve_pngs_flat_objs = [x.reshape(self.input_dim) for x in midcurve_pngs_gray_objs]\n",
    "#             \n",
    "#             profile_pngs_objs = np.array(profile_pngs_flat_objs)\n",
    "#             midcurve_pngs_objs= np.array(midcurve_pngs_flat_objs)\n",
    "#             \n",
    "            profile_pngs_objs = self.process_images(profile_pngs_gray_objs)\n",
    "            midcurve_pngs_objs = self.process_images(midcurve_pngs_gray_objs)\n",
    "            \n",
    "#             train_size = int(len(profile_pngs_objs)*0.7)\n",
    "#             self.x_train = profile_pngs_objs[:train_size]\n",
    "#             self.y_train = midcurve_pngs_objs[:train_size]\n",
    "#             self.x_test = profile_pngs_objs[train_size:]\n",
    "#             self.y_test = midcurve_pngs_objs[train_size:]\n",
    "#             self.autoencoder.fit(self.x_train, self.y_train,\n",
    "#                         epochs=self.epochs,\n",
    "#                         batch_size=5,\n",
    "#                         shuffle=True,\n",
    "#                         validation_data=(self.x_test, self.y_test))\n",
    "                \n",
    "            self.x = profile_pngs_objs\n",
    "            self.y = midcurve_pngs_objs\n",
    "            self.autoencoder.fit(self.x, self.y,\n",
    "                        epochs=self.epochs,\n",
    "                        batch_size=5,\n",
    "                        shuffle=True)                \n",
    "            # Save models\n",
    "            self.autoencoder.save(self.autoencoder_model_pkl)\n",
    "            self.encoder.save(self.encoder_model_pkl)\n",
    "            self.decoder.save(self.decoder_model_pkl)  \n",
    "        else:\n",
    "            # Save models\n",
    "            self.autoencoder = load_model(self.autoencoder_model_pkl)\n",
    "            self.encoder= load_model(self.encoder_model_pkl)\n",
    "            self.decoder = load_model(self.decoder_model_pkl)\n",
    "    \n",
    "    def predict(self, test_profile_images):\n",
    "        png_profile_images = self.process_images(test_profile_images)\n",
    "        encoded_imgs = self.encoder.predict(png_profile_images)\n",
    "        decoded_imgs = self.decoder.predict(encoded_imgs)    \n",
    "        return test_profile_images,decoded_imgs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "448/448 [==============================] - 3s 8ms/step - loss: -3.0615\n",
      "Epoch 2/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -12.8917\n",
      "Epoch 3/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -15.0182A: 0s - loss: -14\n",
      "Epoch 4/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -15.6901\n",
      "Epoch 5/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -16.0536A: 0s - loss: -16.\n",
      "Epoch 6/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.3270\n",
      "Epoch 7/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.4829\n",
      "Epoch 8/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.6132\n",
      "Epoch 9/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.6836\n",
      "Epoch 10/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.7303\n",
      "Epoch 11/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.7797\n",
      "Epoch 12/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.7941\n",
      "Epoch 13/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.8533A: 0\n",
      "Epoch 14/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.8530A: 0s - \n",
      "Epoch 15/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.8786\n",
      "Epoch 16/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9021\n",
      "Epoch 17/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9339\n",
      "Epoch 18/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9222\n",
      "Epoch 19/200\n",
      "448/448 [==============================] - ETA: 0s - loss: -16.9035- ETA: 2s - loss:  - 3s 6ms/step - loss: -16.9317\n",
      "Epoch 20/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9669A: 0s - loss: -16.\n",
      "Epoch 21/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9875A: 0s -\n",
      "Epoch 22/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -16.9858A: 2s - l\n",
      "Epoch 23/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0144\n",
      "Epoch 24/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0265A: 1\n",
      "Epoch 25/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0160\n",
      "Epoch 26/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0227\n",
      "Epoch 27/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0288\n",
      "Epoch 28/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.0541\n",
      "Epoch 29/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.0424A: 0s - loss: \n",
      "Epoch 30/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0639\n",
      "Epoch 31/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0670A: 0s - loss: -17.070\n",
      "Epoch 32/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.0562A: 1\n",
      "Epoch 33/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0653A: 0s - loss: -17.032\n",
      "Epoch 34/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.0831\n",
      "Epoch 35/200\n",
      "448/448 [==============================] - 2s 6ms/step - loss: -17.0817\n",
      "Epoch 36/200\n",
      "448/448 [==============================] - 2s 6ms/step - loss: -17.0888\n",
      "Epoch 37/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1108\n",
      "Epoch 38/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1100A:\n",
      "Epoch 39/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1067\n",
      "Epoch 40/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1267\n",
      "Epoch 41/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1228\n",
      "Epoch 42/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1206\n",
      "Epoch 43/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1439\n",
      "Epoch 44/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1274\n",
      "Epoch 45/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1393\n",
      "Epoch 46/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1529\n",
      "Epoch 47/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1427\n",
      "Epoch 48/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1431\n",
      "Epoch 49/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1377\n",
      "Epoch 50/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1574\n",
      "Epoch 51/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1522\n",
      "Epoch 52/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1687\n",
      "Epoch 53/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1768\n",
      "Epoch 54/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.1483\n",
      "Epoch 55/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1668\n",
      "Epoch 56/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1673\n",
      "Epoch 57/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1691A: 0s - loss: -\n",
      "Epoch 58/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1752\n",
      "Epoch 59/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1809\n",
      "Epoch 60/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1878A: 0s - loss: -17.1\n",
      "Epoch 61/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1819A: 0s - loss:\n",
      "Epoch 62/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1869\n",
      "Epoch 63/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1818\n",
      "Epoch 64/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1926\n",
      "Epoch 65/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2001\n",
      "Epoch 66/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2062\n",
      "Epoch 67/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.1932\n",
      "Epoch 68/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2037\n",
      "Epoch 69/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2050\n",
      "Epoch 70/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2047\n",
      "Epoch 71/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2181\n",
      "Epoch 72/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2178\n",
      "Epoch 73/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2063\n",
      "Epoch 74/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2344\n",
      "Epoch 75/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2001\n",
      "Epoch 76/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2417\n",
      "Epoch 77/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2195\n",
      "Epoch 78/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2358\n",
      "Epoch 79/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2179\n",
      "Epoch 80/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2426\n",
      "Epoch 81/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2447\n",
      "Epoch 82/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2339\n",
      "Epoch 83/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2367\n",
      "Epoch 84/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2351\n",
      "Epoch 85/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2422\n",
      "Epoch 86/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2507\n",
      "Epoch 87/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2312\n",
      "Epoch 88/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2528A\n",
      "Epoch 89/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2485\n",
      "Epoch 90/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2484\n",
      "Epoch 91/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2571\n",
      "Epoch 92/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2560\n",
      "Epoch 93/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2431\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2603\n",
      "Epoch 95/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2554\n",
      "Epoch 96/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2613\n",
      "Epoch 97/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2678\n",
      "Epoch 98/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2538\n",
      "Epoch 99/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2619\n",
      "Epoch 100/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2619\n",
      "Epoch 101/200\n",
      "448/448 [==============================] - ETA: 0s - loss: -17.200 - 2s 5ms/step - loss: -17.2712\n",
      "Epoch 102/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2871\n",
      "Epoch 103/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2864\n",
      "Epoch 104/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2570\n",
      "Epoch 105/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2550\n",
      "Epoch 106/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2759\n",
      "Epoch 107/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2905\n",
      "Epoch 108/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2795\n",
      "Epoch 109/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2882\n",
      "Epoch 110/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2869\n",
      "Epoch 111/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2877\n",
      "Epoch 112/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2882\n",
      "Epoch 113/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2757\n",
      "Epoch 114/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2985\n",
      "Epoch 115/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2849A: 0s - loss: -17.19\n",
      "Epoch 116/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3014\n",
      "Epoch 117/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2824\n",
      "Epoch 118/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2885\n",
      "Epoch 119/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3038\n",
      "Epoch 120/200\n",
      "448/448 [==============================] - 2s 6ms/step - loss: -17.2926\n",
      "Epoch 121/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.2852\n",
      "Epoch 122/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3234A\n",
      "Epoch 123/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2983A: 1s - - ETA: 0s - los\n",
      "Epoch 124/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2892\n",
      "Epoch 125/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2976\n",
      "Epoch 126/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3010\n",
      "Epoch 127/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.2962A: 0s - loss: -17.269\n",
      "Epoch 128/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3100\n",
      "Epoch 129/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3157\n",
      "Epoch 130/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3108\n",
      "Epoch 131/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3138A: 0s - loss: -17.3\n",
      "Epoch 132/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3061A: 0s\n",
      "Epoch 133/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3118\n",
      "Epoch 134/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3043\n",
      "Epoch 135/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3188\n",
      "Epoch 136/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3132\n",
      "Epoch 137/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3158\n",
      "Epoch 138/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3261\n",
      "Epoch 139/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3143\n",
      "Epoch 140/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3171A: 0s - loss: -\n",
      "Epoch 141/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3367\n",
      "Epoch 142/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3276\n",
      "Epoch 143/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3046\n",
      "Epoch 144/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3267\n",
      "Epoch 145/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3344\n",
      "Epoch 146/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3283A: 2s - l\n",
      "Epoch 147/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3371\n",
      "Epoch 148/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3109A: 1s - loss - ETA: 0s - loss: -17.343 - ETA: 0s - loss: \n",
      "Epoch 149/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3271\n",
      "Epoch 150/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3208\n",
      "Epoch 151/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3485\n",
      "Epoch 152/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3250\n",
      "Epoch 153/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3186\n",
      "Epoch 154/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3493\n",
      "Epoch 155/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3358\n",
      "Epoch 156/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3367\n",
      "Epoch 157/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3387\n",
      "Epoch 158/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3550\n",
      "Epoch 159/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3399\n",
      "Epoch 160/200\n",
      "448/448 [==============================] - ETA: 0s - loss: -17.337 - 3s 7ms/step - loss: -17.3400\n",
      "Epoch 161/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3277\n",
      "Epoch 162/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3564\n",
      "Epoch 163/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3479A: 0s - loss: \n",
      "Epoch 164/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3440\n",
      "Epoch 165/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3557\n",
      "Epoch 166/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3423\n",
      "Epoch 167/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3409\n",
      "Epoch 168/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3473\n",
      "Epoch 169/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3605\n",
      "Epoch 170/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3490\n",
      "Epoch 171/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3629A: 2s  - ETA: 1s - loss: -17.5 - ETA: \n",
      "Epoch 172/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3305\n",
      "Epoch 173/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3373\n",
      "Epoch 174/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3555\n",
      "Epoch 175/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3675\n",
      "Epoch 176/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3606\n",
      "Epoch 177/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3573\n",
      "Epoch 178/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3723\n",
      "Epoch 179/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3492A: 0s -\n",
      "Epoch 180/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3575\n",
      "Epoch 181/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3530\n",
      "Epoch 182/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3623\n",
      "Epoch 183/200\n",
      "448/448 [==============================] - 3s 7ms/step - loss: -17.3757A: 0s - loss: -17\n",
      "Epoch 184/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3467- ETA: 0s - loss\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3656\n",
      "Epoch 186/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3792\n",
      "Epoch 187/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3617\n",
      "Epoch 188/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3396\n",
      "Epoch 189/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3645\n",
      "Epoch 190/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3639\n",
      "Epoch 191/200\n",
      "448/448 [==============================] - 2s 6ms/step - loss: -17.3483A: 0s - loss: -\n",
      "Epoch 192/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3670\n",
      "Epoch 193/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3590\n",
      "Epoch 194/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3850\n",
      "Epoch 195/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3670\n",
      "Epoch 196/200\n",
      "448/448 [==============================] - 3s 6ms/step - loss: -17.3649\n",
      "Epoch 197/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3679\n",
      "Epoch 198/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3703\n",
      "Epoch 199/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3656\n",
      "Epoch 200/200\n",
      "448/448 [==============================] - 2s 5ms/step - loss: -17.3898\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile_gray_objs, midcurve_gray_objs = get_training_data()\n",
    "endec = simple_encoderdecoder()\n",
    "endec.train(profile_gray_objs, midcurve_gray_objs)\n",
    "    \n",
    "test_gray_images = random.sample(profile_gray_objs,5)\n",
    "original_profile_imgs,predicted_midcurve_imgs = endec.predict(test_gray_images)\n",
    "plot_results(original_profile_imgs,predicted_midcurve_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
